# Video Caption

## Table of Contents

**TODO:** Use ChatGPT

## Team Information

- Team Members
  - Daniel Mart√≠n ([danitiana98@gmail.com](mailto:danitiana98@gmail.com))
  - Joan Pascual **TODO:** add mail
  - Juan Bacardit **TODO:** add mail
  - Sergi Taramon **TODO:** add mail
- Advisor: Carlos Escolano
- Framework: ``pytorch``

## Introduction

Nowadays we have access to enormous volumes of video data. However, the information contained in these videos is not easily accessible. The goal of this project is to generate a brief description of a small video. This description will be generated by a transformer decoder that will receive encoded videos using a pre-trained model.

These transcriptions have lots of potential applications, such as:

- Video indexing
- Video retrieval
- Video summarization

## Project Goals

Generate a representative brief description of a small video by:

- Use transfer learning though a pre-trained VideoMAE encoder to encode video information.
- Implement transformer decoder to generate captions.
- Train a transformer decoder to generate captions explaining the video content.
- Achieve valid and accurate captions explaining what happens in a given video input.

## Environment Setup

### Dependencies

To run this project you will need to install the following dependencies:

- [Python 3.10](https://www.python.org/downloads/release/python-310)
- [Poetry](https://python-poetry.org/docs/#installation)
- [Project dependencies](./pyproject.toml)
- [Docker](https://docs.docker.com/get-docker/) (optional)

### Cloning and first steps

```shell
git clone https://github.com/dainelli98/video_caption.git
```

After having cloned the repo, if not using docker, create a virtual environment and install Python packages:

```shell
make venv
```

### Installation

Install package via pip using:

```shell
pip install -e .
```

### Poetry

To include a new package to the project it has to be added into the pyproject.toml
under the correct group. Application packages have to be included inside `tool.poetry.dependencies`.
Other packages have to be included in their respective groups like dev, docs or custom groups.

To add a package run `poetry add [options] [--] <name>`. You can indicate the group to
add the dependency to with the option `--group=GROUP`.

To remove a package run `poetry remove [options] [--] <packages>`. You can indicate the group to
remove the dependency from with the option `--group=GROUP`.

To update the `poetry.lock` file, run `poetry lock --no-update`.

Run `poetry list` to get more information about all the commands that poetry can run.

### Generate documentation

Run the following command to generate project API documentation:

```shell
make docs
```

## Running the project

This projects provides 4 commands that can be run and configured usign CLI:
 - ``prepare-dataset``
 - ``train``
 - ``test``
 - ``experiment``

To get information about the arguments that each command accepts, run:

```shell
vid-cap --help

Usage: vid-cap [OPTIONS] COMMAND1 [ARGS]... [COMMAND2 [ARGS]...]...

  Train and evaluate model to obtain video descriptions from videos.

Options:
  --version  Show the version and exit.
  --help     Show this message and exit.
```

### Prepare dataset

To run the script that prepares the video encoding dataset, run:

```shell
vid-cap prepare-dataset
```

To obtain information about the arguments that this command accepts, run:

```shell
vid-cap prepare-dataset --help

Usage: vid-cap prepare-dataset [OPTIONS]

  Prepare dataset with VideoMAE.

Options:
  --data-dir PATH  Data directory
  --help           Show this message and exit.
```

### Train

To run the script to train the model, run:

```shell
vid-cap train
```

To obtain information about the arguments that this command accepts, run:

```shell
vid-cap train --help

Usage: vid-cap train [OPTIONS]

  Train decoder.

Options:
  --warmup-steps INTEGER RANGE    Warmup steps.  [1<=x<=100000]
  --label-smoothing FLOAT RANGE   Label smoothing.  [0<=x<=1]
  --data-dir PATH                 Data directory
  --shuffle                       Shuffle datasets
  --batch-size INTEGER RANGE      Batch size.  [1<=x<=512]
  --n-heads INTEGER RANGE         Number of heads.  [1<=x<=128]
  --n-layers INTEGER RANGE        Number of decoder layers.  [1<=x<=128]
  --use-gpu                       Try to train with GPU
  --epochs INTEGER RANGE          Number of epochs.  [1<=x<=10000]
  --vocab-len INTEGER RANGE       Vocab length. If BPE is used, this is
                                  ignored.  [1<=x<=100000]
  --caps-per-vid INTEGER RANGE    Captions per video used in the dataset.
                                  [1<=x<=20]
  --dropout FLOAT RANGE           Dropout rate.  [0<=x<=1]
  --bpe-num-operations INTEGER RANGE
                                  Number of BPE operations. If not provided,
                                  BPE will not be used.  [1<=x<=100000]
  --help                          Show this message and exit.
```

### Test

To run the script to evaluate the model, run:

```shell
vid-cap test
```

To obtain information about the arguments that this command accepts, run:

```shell
vid-cap test --help

Usage: vid-cap test [OPTIONS]

  Test decoder.

Options:
  --n-heads INTEGER RANGE     Number of heads.  [1<=x<=128]
  --data-dir PATH             Data directory
  --n-layers INTEGER RANGE    Number of decoder layers.  [1<=x<=128]
  --batch-size INTEGER RANGE  Batch size.  [1<=x<=512]
  --use-gpu                   Try to test with GPU
  --experiment-number TEXT    Number timestamp name on experiment folder.
                              [required]
  --help                      Show this message and exit.
```

### Experiment

To run the script that performs an experiment that trains and evaluates a model, run:

```shell
vid-cap experiment
```

To obtain information about the arguments that this command accepts, run:

```shell
vid-cap experiment --help

Usage: vid-cap experiment [OPTIONS]

  Perform experiement.

Options:
  --warmup-steps INTEGER RANGE    Warmup steps.  [1<=x<=100000]
  --label-smoothing FLOAT RANGE   Label smoothing.  [0<=x<=1]
  --data-dir PATH                 Data directory
  --shuffle                       Shuffle datasets
  --batch-size INTEGER RANGE      Batch size.  [1<=x<=512]
  --n-heads INTEGER RANGE         Number of heads.  [1<=x<=128]
  --n-layers INTEGER RANGE        Number of decoder layers.  [1<=x<=128]
  --use-gpu                       Try to train with GPU
  --epochs INTEGER RANGE          Number of epochs.  [1<=x<=10000]
  --vocab-len INTEGER RANGE       Vocab length. If BPE is used, this is
                                  ignored.  [1<=x<=100000]
  --caps-per-vid INTEGER RANGE    Captions per video used in the dataset.
                                  [1<=x<=20]
  --dropout FLOAT RANGE           Dropout rate.  [0<=x<=1]
  --bpe-num-operations INTEGER RANGE
                                  Number of BPE operations. If not provided,
                                  BPE will not be used.  [1<=x<=100000]
  --help                          Show this message and exit.
```

## Dataset. MSR-VTT 10K

The dataset selected was MSR-VTT 10K. MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering the most comprehensive categories and diverse visual content, and representing the largest dataset in terms of sentence and vocabulary. Each clip is annotated with about 20 natural sentences by 1,327 AMT workers. Total captions contain ~29k unique words.

![Dataset](./report/images/dataset.png)

### Dataset metadata

All video info and caption sentences are stored using the JSON file format. All data share the basic data structure below:

```json
{
  "info" : info,
  "videos": [video],
  "sentences": [sentence],
}

info {
  "year" : str,
  "version" : str,
  "description": str,
  "contributor": str,
  "data_created": str,
}

video {
  "id": int,
  "video_id": str,
  "category": int,
  "url": str,
  "start time": float,
  "end time": float,
  "split": str,
}

sentence {
  "sen_id": int,
  "video_id": str,
  "caption": str,
}
```

Example:

```json
{
  "info":
  {
    "contributor": "Microsoft MSM group",
    "data_created": "2016-04-14 14:30:20",
    "version": "1.0",
    "description": "This is 1.0 version of the 2016 MSR-VTT dataset.", "year": "2016"
  },
  "videos":
  [
    {
      "category": 9,
      "url": "https://www.youtube.com/watch?v=9lZi22qLlEo",
      "video_id": "video0",
      "start time": 137.72,
      "end time": 149.44, "split": "train", "id": 0},
    {
      "category": 16,
      "url": "https://www.youtube.com/watch?v=w4JM08PDEng",
      "video_id": "video1",
      "start time": 184.33,
      "end time": 206.89,
      "split": "train",
      "id": 1
    }
    ...
  ],
  "sentences":
  [
    {
      "sen_id": 0,
      "video_id": "video0",
      "caption": "A person is drawing a picture."
    },
    {
      "sen_id": 1,
      "video_id": "video0",
      "caption": "A person is drawing a picture."
    }
    ...
  ]
```

Using the above data structure, we can easily download the videos from YouTube using the video_id and the start and end time. We can also download the captions using the video_id and the caption.

Also, the videos can be found already extracted from YouTube in [Mediafire](https://www.mediafire.com/folder/h14iarbs62e7p/shared).

### Data splits

| Splits | Init     | End      | # Videos | # Captions/Video |
|--------|----------|----------|----------|------------------|
| Train  | video0   | video6512| 6513     | 20               |
| Val    | video6513| video7009| 497      | 20               |
| Test   | video7010| video9999| 2990     | 20               |

## Methods/Algorithms Used

### Video Encoding

We used the predefined distribution of the original dataset (Train: 6513, Val: 497 and Test: 2990). In order to transform the original data set that contains videos and related captions, we decided to use an existing model as a transformer encoder in order to generate the corresponding embeddings for the videos and correlate those video embeddings with the captions.

We implemented a script [prepare_dataset](./vid_cap/scripts/prepare_dataset.py) that allows to generate video encoddings with a giving sampling value.

#### Frame extraction

Our initial implementation extracts 16 evenly spaced frames from each video.

We considered iterating and use ``FFmpeg`` to extract the 16 most representative frames from each video, but we decided to keep the first approach as the results were good enough.

![Frames](./report/images/frames.png)

#### VideoMAE as video encoder

VideoMAE, standing for Video Masked Autoencoder, is a self-supervised video pre-training approach designed for efficient learning from video data. It was proposed in the paper ["VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"](https://arxiv.org/abs/2203.12602).

Key characteristics and features of VideoMAE include:

- High masking ratio (between 90% and 95%) in its training process. Despite the extensive masking, VideoMAE still performs well due to the temporally redundant content in videos, which allows for meaningful reconstruction from limited data.
- Demonstrates impressive results on small datasets (around 3k-4k videos) without needing additional data. This makes it a data-efficient learner in the field of self-supervised video pre-training.
- Emphasis on Data Quality: The authors of VideoMAE found that the quality of the data is more important than the quantity for self-supervised video pre-training. The domain shift, or the difference between the pre-training and target datasets, is also an important factor to consider.
- Effective Video Representations: By making video reconstruction a challenging task, VideoMAE encourages the extraction of more effective video representations during the pre-training process. This makes it particularly useful for tasks that require understanding and interpreting video content.
- When implemented with the vanilla ViT (Vision Transformer) backbone, achieved high performance on various video understanding benchmarks, including Kinects-400, Something-Something V2, UCF101, and HMDB51, without using any extra data.

We used the implementation that could be found on [Hugging Face](https://huggingface.co/docs/transformers/main/model_doc/videomae).

![VideoMAE](./report/images/videomae_architecture.jpeg)

Utilizing the VideoMAE model, we have generated several comprehensive datasets. Each dataset is characterized by unique parameters such as video frame rate, embedding sampling period (i.e., selecting one vector every X frames), tensor representation of each video, and overall dataset size.

#### Video encoding postprocessing

Even if the original videos do amount to no more than 7 GB, the generated embeddings are much larger. This is due to the fact that the original videos leverage mp4 compression, which would not take effect when loading memory.

This raised the need to reduce the size of the generated embeddings for easy experimentation.

To reduce dataset size, we used 2 different methodologies, reducing float precision and embedding sampling.

First, we reduced the precision of the values in the embedding by casting them from ``float32`` to ``float16``.

The comprehensive embedding representation derived from the VideoMAE model is a tensor of dimensions [1,1568, 768]. To streamline usage during the model development phase, we generated several datasets by performing different levels of sampling (taking one of each ``n`` vectors per embedding). This approach involved utilizing a low sampling representation during the initial stages of model development and stabilization. Once the model was stable, larger representation datasets were employed during the experimental or training execution phase. This strategy ensured manageable data handling without compromising the robustness of our experiments too much.

The experiments performed posterior to the generation of the datasets showed that this strategy produced datasets that contained representative information.

#### Final preprocessed datasets

The table below provides a summary of the generated datasets:

- Original embeddings: 32-bit floating point representation of the embeddings without any sampling.

- Reduced precision embeddings: 16-bit floating point representation of the embeddings without any sampling.

- Sampled embeddings: 16-bit floating point representation of the embeddings with sampling at different levels (2, 4, 8, 16, 32).

- No-sampling datasets: These datasets were generated with a video frame rate of 27 frames/sec. The tensor representation for each video is [1,1568,768] with int64 type, and the overall size of the dataset is 41.50 GB. The dataset is saved under the name "dataset".

| Embedding sampling period (pick one vector every X) | Embed dims/video | Tensor Dtype | Dataset Size |
| --------------------------------------------------- | ---------------- | ------------ | ------------ |
| No-sampling                                         | (1,1568,768)     | float32      | 41.50 GB     |
| No-sampling                                         | (1,1568,768)     | float16      | 20.74 GB     |
| 2                                                   | (1,784,768)      | float16      | 10.38 GB     |
| 4                                                   | (1,392,768)      | float16      | 10.13 GB     |
| 8                                                   | (1,196,768)      | float16      | 2.6 GB       |
| 16                                                  | (1,98,768)       | float16      | 1.3 GB       |
| 32                                                  | (1,49,768)       | float16      | 645 MB       |

### Transformer Decoder

**TODO:**

#### Model Architecture

**TODO:**

#### General data flow

**TODO:**

![General data flow](./report/images/general_flow.png)


### Model training

**TODO:**

### Model evaluation

**TODO:**

## Experiments

With the model implementation finished we proceeded to perform several experiments to improve the model performance as much as possible while getting answering other questions that arose in the previous steps of the process.

### Questions to answer

After the initial stages of the project, we identifyed several questions that we wanted to answer with the experiments:

- Is it possible to obtain a model that can generate a caption for a video with at least some information about the content of the video usign a VideoMAE generated embedding?
- Can we obtain a model that can generate a caption for a video usign embeddings with reduced precision and information (reduced sampling)?
- Which hyperparameters can be provided to the model to obtain the best results?

### Experiment definition

We implemented 3 different scripts to perform the experiments:
  - [train](./vid_cap/scripts/train.py): Trains a model ans saves model, training information and vocabulary.
  - [test](./vid_cap/scripts/test.py): Loads model and vocabulary and uses test data to evaluate model performance.
  - [experiment](./vid_cap/scripts/experiment.py): Runs train and test. At the end stores experiment information and adds it to the experiment tracking file.

All scripts can be observed with ``Tensorboard``.

To perform the experiments we used the ``experiment`` script providing several paramerter combinations. The parameters used for the experiments are the following:

- Input data:
  - Encoded data: The VideoMAE generated embeddings. We tested with different sampling values.
  - Captions per video: Number of captions per video to use in training and validation.
- Model hyperparameters:
  - Number of heads: Number of heads in the multi-head attention layers.
  - Number of layers: Number of layers Transformer decoder of the model.
- Training hyperparameters:
  - Batch size: Number of samples per batch.
  - Shuffle: Whether to shuffle the dataset or not.
  - Label smoothing: Label smoothing value.
  - Dropout: Dropout value.
  - Embeddings: Embedding size.
  - Warmup steps: Number of warmup steps.

### Experimental process

**TODO:**

## Results

**TODO**

### Extended results

The following table contains the most relevant experiments performed with the model, providing different combinations of parameters and inputs.

|   Batch size | Shuffle   |   # Heads |    # Layers |   Vocab length |   Captions per video |   Label smoothing |   Dropout |   Embeddings |   Warmup steps |   Validation BLEU score |   Train loss |   Validation loss |   Test BLEU score |
|-------------:|:----------|----------:|-----------:|----------------:|---------------------:|------------------:|----------:|-------------:|---------------:|------------------------:|-------------:|------------------:|------------------:|
|           64 | True      |         4 |          2 |            8000 |                   10 |               0.1 |       0.1 |           98 |           6000 |                  0.0579 |       2.6251 |            4.3751 |            0.3584 |
|           64 | True      |         4 |          2 |           15000 |                   10 |               0.1 |       0.1 |           98 |           6000 |                  0.0594 |       2.7463 |            4.5089 |            0.3363 |
|           64 | True      |         4 |          2 |           10000 |                   10 |               0.1 |       0.1 |           98 |           6000 |                  0.0554 |       2.6837 |            4.4896 |            0.3214 |
|           64 | True      |         4 |          2 |            9647 |                    3 |               0.1 |       0.1 |           98 |           6000 |                  0.0636 |       2.4494 |            4.3972 |            0.3005 |
|           64 | True      |         4 |          2 |           10000 |                    8 |               0.1 |       0.1 |           98 |           6000 |                  0.0649 |       2.6375 |            4.418  |            0.2969 |
|           64 | True      |         4 |          2 |            6000 |                   10 |               0.1 |       0.1 |           98 |           6000 |                  0.0573 |       2.5803 |            4.3215 |            0.2931 |
|           64 | True      |         4 |          2 |           10000 |                   10 |               0.1 |       0.1 |           49 |           6000 |                  0.059  |       2.6956 |            4.4632 |            0.2929 |
|           64 | True      |         4 |          2 |           10000 |                   10 |               0.1 |       0.1 |          784 |           6000 |                  0.0497 |       2.6755 |            4.5463 |            0.2879 |
|           64 | True      |         4 |          2 |           10000 |                    5 |               0.1 |       0.1 |           98 |           6000 |                  0.0632 |       2.5273 |            4.3388 |            0.2623 |
|           64 | True      |         4 |          2 |           10000 |                   15 |               0.1 |       0.1 |           98 |           6000 |                  0.0478 |       2.7495 |            4.6483 |            0.26   |
|           64 | True      |         4 |          2 |           10000 |                   10 |               0.1 |       0.1 |          196 |           6000 |                  0.059  |       2.6644 |            4.5092 |            0.2591 |
|           64 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0   |           98 |           6000 |                  0.0551 |       2.4428 |            4.4057 |            0.249  |
|           64 | True      |         4 |          2 |           10000 |                    4 |               0.1 |       0.1 |           98 |           6000 |                  0.0599 |       2.5302 |            4.3898 |            0.2326 |
|           64 | True      |         4 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           6000 |                  0.051  |       2.3263 |            4.4374 |            0.2299 |
|           32 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0   |           98 |           6000 |                  0.0467 |       2.737  |            4.6148 |            0.2257 |
|          100 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0   |           98 |           6000 |                  0.0504 |       2.153  |            4.3464 |            0.2154 |
|           64 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0   |           98 |           4000 |                  0.0433 |       2.3682 |            4.4897 |            0.2128 |
|           64 | True      |         3 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           6000 |                  0.0512 |       2.4137 |            4.3966 |            0.208  |
|           64 | True      |         4 |          2 |           10000 |                   20 |               0.1 |       0.1 |           98 |           6000 |                  0.0377 |       2.752  |            4.8468 |            0.2019 |
|           64 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           6000 |                  0.0501 |       2.436  |            4.4146 |            0.1887 |
|          100 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0   |           98 |           4000 |                  0.0407 |       2.2216 |            4.3759 |            0.1814 |
|           64 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0.0431 |       2.5554 |            4.5715 |            0.1739 |
|           64 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0.2 |           98 |           4000 |                  0.0423 |       2.6161 |            4.6079 |            0.1731 |
|           64 | True      |         4 |          2 |           10000 |                   10 |               0.1 |       0.1 |          392 |           6000 |                  0.0482 |       2.7182 |            4.6157 |            0.17   |
|           64 | True      |         8 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           6000 |                  0.0419 |       2.3638 |            4.4976 |            0.1646 |
|           64 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0.0415 |       2.5694 |            4.5844 |            0.1643 |
|           32 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0   |           98 |           4000 |                  0.0276 |       3.1268 |            4.9462 |            0.1505 |
|           64 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0.2 |           98 |           4000 |                  0.0306 |       2.5778 |            4.7218 |            0.1286 |
|           64 | True      |         4 |          3 |            8073 |                    2 |               0.1 |       0.1 |           98 |           6000 |                  0.0272 |       2.5867 |            4.7824 |            0.1162 |
|           32 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0.0197 |       3.0811 |            4.9345 |            0.1031 |
|           64 | True      |         4 |          2 |            5966 |                    1 |               0.1 |       0.1 |           98 |           6000 |                  0.0269 |       2.3017 |            4.4986 |            0.0908 |
|           64 | True      |         6 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           6000 |                  0.0437 |       2.3596 |            4.5085 |            0.0774 |
|          100 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0   |           98 |           4000 |                  0.025  |       4.0037 |            4.848  |            0.0654 |
|           32 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0.0145 |       3.2447 |            4.9976 |            0.0569 |
|           32 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0   |           98 |           4000 |                  0.0321 |       4.4453 |            5.0571 |            0.0482 |
|           64 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0   |           98 |           4000 |                  0.0395 |       3.8939 |            4.6937 |            0.0378 |
|           64 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0.0256 |       4.4217 |            4.9525 |            0.0289 |
|           64 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0.0221 |       4.6194 |            4.9522 |            0.0288 |
|           64 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0.2 |           98 |           4000 |                  0.0139 |       4.5701 |            5.0922 |            0.0203 |
|           64 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0.2 |           98 |           4000 |                  0.017  |       4.5684 |            5.1323 |            0.018  |
|           32 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0      |       5.1691 |            5.6516 |            0.0016 |
|           32 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0.2 |           98 |           4000 |                  0      |       3.3888 |            5.5987 |            0.0012 |
|           64 | True      |         4 |          4 |            8073 |                    2 |               0.1 |       0.1 |           98 |           6000 |                  0      |       3.768  |            6.1415 |            0      |
|           64 | True      |         4 |          6 |            8073 |                    2 |               0.1 |       0.1 |           98 |           6000 |                  0      |       3.784  |            6.4769 |            0      |
|           64 | True      |         4 |         10 |            8073 |                    2 |               0.1 |       0.1 |           98 |           6000 |                  0      |       3.7885 |            6.3405 |            0      |
|           32 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0.2 |           98 |           4000 |                  0      |       5.3778 |            5.8296 |            0      |
|          100 | False     |         4 |          4 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0      |       6.0279 |            6.1063 |            0      |
|           32 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0      |       5.3581 |            5.9211 |            0      |
|           32 | False     |         2 |          2 |            8073 |                    2 |               0.1 |       0.2 |           98 |           4000 |                  0      |       5.621  |            5.9314 |            0      |
|          100 | False     |         4 |          4 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0      |       6.0279 |            6.1063 |            0      |
|          100 | True      |         4 |          4 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0      |       3.353  |            5.7906 |            0      |
|           32 | True      |         2 |          2 |            8073 |                    2 |               0.1 |       0.2 |           98 |           4000 |                  0      |       3.4094 |            5.6264 |            0      |
|          100 | True      |         4 |          4 |            8073 |                    2 |               0.1 |       0.1 |           98 |           4000 |                  0      |       3.7105 |            6.1041 |            0      |

### Results Summary

**TODO:** Summary of results

## Conclusions

**TODO:**

## Next Steps

**TODO:**

## References

- [MSR-VTT: A Large Video Description Dataset for Bridging Video and Language](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf)
- [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602)
- [VideoMAE Huggingface](https://huggingface.co/docs/transformers/main/model_doc/videomae)

**TODO:**
